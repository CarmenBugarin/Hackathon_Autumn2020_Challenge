{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scikit_learn_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qet_C1Qljuxx"
      },
      "source": [
        "<p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/scikit-learn-logo-small.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Introduction to Machine Learning with Scikit-learn</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqfm6CpLkS_v"
      },
      "source": [
        "## **Getting started**\n",
        "\n",
        "Many Python libraries provide reliable implementations of machine learning algorithms. One of the most popular is Scikit-learn (https://scikit-learn.org/stable/index.html), an open-source package that provides efficient implementations of a large number of machine learning methods and algorithms. \n",
        "\n",
        "Scikit-learn is characterized by a clean API, as well as by handy and complete online documentation. A benefit of this uniform API is that once you understand the basic use and syntax for one type of model, switching to a new model is pretty straightforward. \n",
        "\n",
        "A guide illustrating some of the Scikit-learn main features can be found [here](https://scikit-learn.org/stable/getting_started.html).\n",
        "\n",
        "In this tutorial, we will learn how to use Scikit-learn to perform a very basic machine learning task involving supervised learning. Let's start!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QNxZDxiDe19"
      },
      "source": [
        "# Importing Python libraries\n",
        "As we saw in the Python course, it is a good practice first to import all the Python libraries that we are going to use in the script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3MtKIHolcwq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKCTNwFCEQd_"
      },
      "source": [
        "# Download dataset\n",
        "\n",
        "Now, let's download and have a quick look at the data we will use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_MqLKvKMuJJ"
      },
      "source": [
        "dataset_url = 'https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/dataset.csv'\n",
        "dataset = pd.read_csv(dataset_url)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyrDl0l2gVe"
      },
      "source": [
        "In our dataset, we have data from about 200 people telling:\n",
        "\n",
        "*   The mean time spent washing their hands;\n",
        "*   The mean number of times that they touched their face per hour;\n",
        "*   And if they got a virus in the following 2 months.\n",
        "\n",
        "Based on this data, we want to develop a model that would perform predictions in new data. To accomplish this, we will use the **machine learning workflow**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkICW-xsFlK0"
      },
      "source": [
        "# **Machine learning workflow**\n",
        "For this tutorial, we will use the workflow presented during the theory presentation.\n",
        "\n",
        "\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/workflow.png\" width=\"600\"></center>\n",
        "\n",
        "\n",
        "\n",
        "The machine learning workflow tries to give us a recipe to follow when working with a machine learning project. The first step is to perform the **problem formulation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YU56yRWZPNV"
      },
      "source": [
        "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/problem.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p> **Problem formulation**\n",
        "\n",
        "First, we need to frame our machine learning problem. In this step, we need to define the features that we will use, the target variable or label that we want to predict, and finally, the machine learning task.\n",
        "\n",
        "Give a try to create a straight forward sentence presenting our project scope by yourselves. When you are done, check our example of the framing below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnw9EbBrKjYY"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5B9opQVINP9"
      },
      "source": [
        "> *Given how often people touch their own faces and how long they wash their hands, predict if they get ill in the next 2 months.*\n",
        "\n",
        "*   Task: Classification\n",
        "*   Features: Washing hand time and how many times touch the face\n",
        "*   Target: Outcome in the next 2 months  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op818zaWJoGi"
      },
      "source": [
        "For convenience, let's store our features and targets in lists to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3rELsmM3vY"
      },
      "source": [
        "features = ['washing hands', 'touching face']\n",
        "targets = ['outcome']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4UInw2nGtD"
      },
      "source": [
        "Remember, framing your scope concisely and comprehensively is very important to guarantee that everyone involved in your project understands your goals.\n",
        "For more information about this step, please, check this [material](https://developers.google.com/machine-learning/problem-framing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qah6FegWldT6"
      },
      "source": [
        "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/data.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>  **Data preparation**\n",
        "\n",
        "In this step, we perform most of the exploratory data analyses and deal with the data's inconsistencies. As we saw in the previous course, Pandas is an excellent tool to perform this step when working with tabular data.\n",
        "\n",
        "In our case, our dataset is already cleaned and validated for our task. Let's check how it looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUCGGo1F8qLf"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuHnvw7Z85cC"
      },
      "source": [
        "To have more flexibility to plot some graphs, we opt to use the data in the Numpy array format. [NumPy](https://numpy.org/) is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
        "\n",
        "An introduction to Numpy is out of the scope of today's notebook. If you want to get a better understanding of this library capabilities, please check out this [Quickstart Tutorial](https://numpy.org/devdocs/user/quickstart.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJlJYCod9X6v"
      },
      "source": [
        "Back to our project, in order to extract the Numpy arrays from a Pandas Dataframe, we used the \".values\" call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7weTVgzE7P65"
      },
      "source": [
        "X = dataset[features].values\n",
        "y = dataset[targets].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQo-0kCu5Tvk"
      },
      "source": [
        "By using this format, we have the following feature matrix and target vector.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/features_targets.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVM6VIGcNmJp"
      },
      "source": [
        "print(f'Features type: {type(X)}')\n",
        "print(f'Target type: {type(y)}')\n",
        "\n",
        "print(f'Features dimensions: {X.shape}')\n",
        "print(f'Target dimensions: {y.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckcbFSzY9hcK"
      },
      "source": [
        "\n",
        "Besides Numpy, the [Matplotlib](https://matplotlib.org/) library is also an excellent tool that allows the creation of different figures and plots. This visualisation tool is handy to get a better insight into how the features and labels are related. For a great guide about this tool, please check out [this material](https://realpython.com/python-matplotlib-guide/).\n",
        "\n",
        "So let's try to visualise how our features are distributed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keUon2K2-j6u"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], marker='.', color='black')\n",
        "plt.xlabel('Washing hands (seconds)')\n",
        "plt.ylabel('Face touches (per hour)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbEb1J3Lmx8"
      },
      "source": [
        "Since we are trying to perform classification, let's have a look at how the different categories are scattered in our plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME7GCN2OHik"
      },
      "source": [
        "healthy_index = np.where(y=='healthy')\n",
        "ill_index = np.where(y=='ill')\n",
        "\n",
        "plt.scatter(X[healthy_index, 0], X[healthy_index, 1], marker='o', color='blue', label='healthy')\n",
        "plt.scatter(X[ill_index, 0], X[ill_index, 1], marker='x', color='red', label='ill')\n",
        "plt.xlabel('Washing hands (seconds)')\n",
        "plt.ylabel('Face touches (per hour)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-J1kEuLeXsz"
      },
      "source": [
        "### **Splitting Your Data**\n",
        "In the presentation, we introduced the idea of dividing your data set into two subsets:\n",
        "\n",
        "*   Training set — a subset to train our model.\n",
        "*   Test set — a subset to test our trained model.\n",
        "\n",
        "The creation of a test set is essential to have an idea of how your machine learning solution will perform in unseen data.\n",
        "\n",
        "In order to get an unbiased measure, we must **never train on test data**. If you see surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set. So please, double-check it.\n",
        "\n",
        "Fortunately, Scikit-learn provided us with a very handful function to divide our original dataset. Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkfjmvftmeRj"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biu7_ekr3FKi"
      },
      "source": [
        "**NOTE**: Before continue, please, check out the **Scikit-learn documentation** about the train_test_split() function in this [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). As a programmer, a crucial skill is to know where you can find out about how to implement what you want. Some great tools to help at this point is **Google** and the **Python library documentation** that you are working with! So let's get familiar with them! ^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ibDX6jLbbdYg"
      },
      "source": [
        "A machine learning project is characterised by a very iterative process. There are several combinations of transformations and models that can be used to perform our task. In order to evaluate these different choices to then apply the best approach in the new unseen data (aka test set), we usually divide our training set again, into the validation set and training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oYRgdW5MbdYg"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "frUPSf1fbdYk"
      },
      "source": [
        "Finally, let's check the size of our new sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd4IvNlm_RSy"
      },
      "source": [
        "print(f'Training set size: {X_train.shape[0]}')\n",
        "print(f'Validation set size: {X_val.shape[0]}')\n",
        "print(f'Test set size: {X_test.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unGyAViw2h4x"
      },
      "source": [
        "There are other different ways to split your dataset. Some of them are very useful when you have a low number of samples or some confounding variables in your dataset. The aim of performing this step is to evaluate your machine learning models' results without the risk of having an undesired bias.\n",
        "\n",
        "This division of the data and the evaluation process is called **cross-validation**.\n",
        "\n",
        "For more information about cross-validation:\n",
        "\n",
        "- https://developers.google.com/machine-learning/data-prep\n",
        "- https://scikit-learn.org/stable/modules/cross_validation.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1ZmI0FWwbdYp"
      },
      "source": [
        "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/feature_engineering.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>   **Feature Engineering**\n",
        "\n",
        "Feature engineering refers to transformations performed on the raw data to generate our features.\n",
        "\n",
        "Dimensionality reduction, feature selection, and feature scaling/normalizing are a few of the transformation that can be applied transformations.\n",
        "\n",
        "But keep in mind that feature engineering is not restricted to it. Usually, for each application domain, particular transformations could be applied to the raw data to facilitate the task for our machine learning model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k9Ss-C2iTlV"
      },
      "source": [
        "In our tutorial, we will **normalize/scale our data**.\n",
        "\n",
        "As we saw before, the variable \"Washing hands\" have values in a range of 0.72 s to 24 s, and our variable \"Face touches\" is between 9 and 36.\n",
        "\n",
        "The range of these two variables is not very different. Still, for some types of models, this difference might make the model perform not very well by making it to give more importance to the variable with the greater values.\n",
        "\n",
        "To prevent this, usually, we normalize our variable to a common range. In our example, we will use a **min-max normalization** where we squash the values of the variables to a range of 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbVOtZzziacK"
      },
      "source": [
        "# First, we create an scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Using the method .fit, our scaler will learn statistics about our training set to perform the scaling.\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Finally, we scale our data\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Note: Did you notice that we only used the training set to get the statistics for our scaler?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BQJBnVxAbdYs"
      },
      "source": [
        "Depending on the scenario, this kind of transformation is not the most appropriate. For example, if you have outliers present in your dataset, this kind of transformation might cause some harm to the model's performance. For this reason, it might be necessary to try several different approaches to try to find a good one.\n",
        "\n",
        "Let's check how our data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Pou1HBkBqn"
      },
      "source": [
        "plt.scatter(X_train_normalized[:, 0], X_train_normalized[:, 1], marker='.', color='k')\n",
        "plt.scatter(X_val_normalized[:, 0], X_val_normalized[:, 1], marker='.', color='k')\n",
        "plt.scatter(X_test_normalized[:, 0], X_test_normalized[:, 1], marker='.', color='k')\n",
        "plt.xlabel('Washing hands (seconds)')\n",
        "plt.ylabel('Face touches (per hour)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEMz7uxqkoB6"
      },
      "source": [
        "Note: there are some values outside the [0, 1] range. Any idea why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mnINHzJXg2S"
      },
      "source": [
        "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/training.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>   **Training model**\n",
        "\n",
        "\n",
        "Training a model means learning (determining) good values for all model's parameters from labelled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss.\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, the loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of parameters that have low loss, on average, across all examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQM7yNcloBa"
      },
      "source": [
        "### **Choosing your model**\n",
        "\n",
        "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
        "\n",
        "Different estimators are better suited for different types of data and different problems.\n",
        "\n",
        "Fortunately, Scikit-learn provided us with a cheat-sheet with some suggestions about which model to use on which occasion. Let's try it!\n",
        "\n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\">\n",
        "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\">\n",
        "</a>\n",
        "Source: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXdpyPuVa-WF"
      },
      "source": [
        "In our case, we have about 200 samples, and we are performing a classification.\n",
        "\n",
        "Then, the recommended model is [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
        "\n",
        "Before continue, please, check out its documentation to get familiar with the API and examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgu4DYtO_IYD"
      },
      "source": [
        "To start, let's create an instance of this model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia9ZpgngApMI"
      },
      "source": [
        "clf = LinearSVC(random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjn__LP_NmK"
      },
      "source": [
        "Now, we can train our model on our training set using the method \".fit()\". \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2hfcwRRBIkZ"
      },
      "source": [
        "clf.fit(X_train_normalized, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhrbYD7_gSQ"
      },
      "source": [
        "And that is it! We trained our machine learning model.\n",
        "\n",
        "Let's make some predictions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxzuP5nZmaYT"
      },
      "source": [
        "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/figures/evaluation.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>   **Model evaluation**\n",
        "\n",
        "Fitting a model to some data does not entail that it will predict well on unseen data. This needs to be directly evaluated. We have just seen the train_test_split helper that splits a dataset into train, validation and test sets. Still, Scikit-learn provides many other tools for model evaluation, in particular for cross-validation.\n",
        "\n",
        "Let's start making some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmWxfECH7l2G"
      },
      "source": [
        "pred = clf.predict(X_val_normalized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEgZZNJmmCWT"
      },
      "source": [
        "Now, let's check them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q7AH4j179vL"
      },
      "source": [
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-MZnbNcmKBr"
      },
      "source": [
        "And now, let's check the ground truth!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtRWzNLT8GAR"
      },
      "source": [
        "print(y_val.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEaWjdeI9E5W"
      },
      "source": [
        "Since it is a lot of samples being predicted, let's summarise it into some performance metrics. The Scikit-Learn offer a wide number of options of metrics. They are specific for which machine learning task we are performing. In our case, we are developing a classifier, so a commonly used metric is the accuracy. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        "\n",
        "You can check for other metrics from Scikit-learn [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFR58KOGBWh3"
      },
      "source": [
        "acc = accuracy_score(y_val, pred)\n",
        "print(f'Model Accuracy: {acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5-wDHiz_B-9"
      },
      "source": [
        "Accuracy comes out to 0.96875, or 96.875% (31 correct predictions out of 32 total examples from the validation set). That means our classifier is doing a decent job of identifying people in risk, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebREAbzFnBRZ"
      },
      "source": [
        "# **STOP!**\n",
        "\n",
        "At this point, try to go back to the definition of our model and the scaler that we used and try to change some of their parameters (please, check their documentation for this). After changed it, run the normalization and the model training again, and check your performance on the validation set.\n",
        "\n",
        "After you find the best performance, let's try it on the test set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfZK7ySnukT"
      },
      "source": [
        "Let's get the performance on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evXhbxoX8D9G"
      },
      "source": [
        "test_pred = clf.predict(X_test_normalized)\n",
        "print(test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAMfdiXLn6N4"
      },
      "source": [
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f'Model Final Accuracy: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viPYl4oepZUX"
      },
      "source": [
        "Nice! Let's see the prediction for a new subject."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAbdY4_kpibH"
      },
      "source": [
        "new_subj = np.array([[20,10]])\n",
        "new_subj_normalized = scaler.transform(new_subj)\n",
        "new_subject_pred = clf.predict(new_subj_normalized)\n",
        "print(new_subject_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFZq6p7koPeE"
      },
      "source": [
        "Well done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67QWiNqRFS0f"
      },
      "source": [
        "More information about model evaluation:\n",
        "*   https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "*   https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cztx9KMme9q"
      },
      "source": [
        "## **Finalising**\n",
        "### **Saving model and predictions**\n",
        "\n",
        "Great! After training our models and scaler, we want to keep them to use it in the future. For this reason, let's save our model in a persistent format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB4atvRbmk84"
      },
      "source": [
        "# We will use the joblib library for this\n",
        "from joblib import dump\n",
        "\n",
        "dump(scaler, 'scaler.joblib') \n",
        "dump(clf, 'classifier.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDH_4K_Vo2H9"
      },
      "source": [
        "Now let's load them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o3B2UeTo0lz"
      },
      "source": [
        "from joblib import load\n",
        "\n",
        "new_scaler = load('scaler.joblib') \n",
        "new_clf = load('classifier.joblib') \n",
        "\n",
        "# Let's check again in our new subject\n",
        "new_subj_normalized = new_scaler.transform(new_subj)\n",
        "new_subject_pred = new_clf.predict(new_subj_normalized)\n",
        "print(new_subject_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fgDIZ3Bijy"
      },
      "source": [
        "Tip: Saving models to combine them later will be handy when working in our challenge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZst-KgMMqbE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo556U-FsWT4"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "In the notebook, we used the 'washing hands' and 'touching face' features to perform the classification. Let's try to predict the outcomes using only one of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2U9JM3Vsu0l"
      },
      "source": [
        "# Data preparation\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/dataset.csv')\n",
        "\n",
        "# #####################\n",
        "# CHANGE THE CODE BELOW\n",
        "# #####################\n",
        "\n",
        "features = ['washing hands', 'touching face']\n",
        "\n",
        "# #####################\n",
        "\n",
        "targets = ['outcome']\n",
        "\n",
        "X = dataset[features].values\n",
        "y = dataset[targets].values\n",
        "\n",
        "# Spliting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Feature engineering\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Training model\n",
        "clf = LinearSVC(random_state=0)\n",
        "clf.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "pred = clf.predict(X_val_normalized)\n",
        "\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(f'Model Accuracy: {acc}')\n",
        "\n",
        "test_pred = clf.predict(X_test_normalized)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f'Model Final Accuracy: {test_acc}')\n",
        "\n",
        "# Saving model\n",
        "dump(scaler, 'ex1_scaler.joblib') \n",
        "dump(clf, 'ex1_classifier.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7kfArwbkmVJ"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "In the notebook, we used a min-max scaler. Let's try a different feature engineering method from here https://scikit-learn.org/stable/modules/preprocessing.html ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd0ZVhypklQR"
      },
      "source": [
        "# Data preparation\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/dataset.csv')\n",
        "\n",
        "features = ['washing hands', 'touching face']\n",
        "targets = ['outcome']\n",
        "\n",
        "X = dataset[features].values\n",
        "y = dataset[targets].values\n",
        "\n",
        "# Spliting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Feature engineering\n",
        "# #####################\n",
        "# CHANGE THE CODE BELOW\n",
        "# #####################\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# #####################\n",
        "# Training model\n",
        "clf = LinearSVC(random_state=0)\n",
        "clf.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "pred = clf.predict(X_val_normalized)\n",
        "\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(f'Model Accuracy: {acc}')\n",
        "\n",
        "test_pred = clf.predict(X_test_normalized)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f'Model Final Accuracy: {test_acc}')\n",
        "\n",
        "# Saving model\n",
        "dump(scaler, 'ex2_scaler.joblib') \n",
        "dump(clf, 'ex2_classifier.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkS1LZn9OB1T"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVI2_4YxF-CV"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "In the notebook, we used a LinearSVM model. Let's try a different model from here https://scikit-learn.org/stable/supervised_learning.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMI8m5t5GDvd"
      },
      "source": [
        "# Data preparation\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day2notebooks/dataset.csv')\n",
        "\n",
        "features = ['washing hands', 'touching face']\n",
        "targets = ['outcome']\n",
        "\n",
        "X = dataset[features].values\n",
        "y = dataset[targets].values\n",
        "\n",
        "# Spliting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Feature engineering\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Training model\n",
        "# #####################\n",
        "# CHANGE THE CODE BELOW\n",
        "# #####################\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC(random_state=0)\n",
        "\n",
        "# #####################\n",
        "clf.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "pred = clf.predict(X_val_normalized)\n",
        "\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(f'Model Accuracy: {acc}')\n",
        "\n",
        "test_pred = clf.predict(X_test_normalized)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f'Model Final Accuracy: {test_acc}')\n",
        "\n",
        "# Saving model\n",
        "dump(scaler, 'ex3_scaler.joblib') \n",
        "dump(clf, 'ex3_classifier.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UltYSZSr-22"
      },
      "source": [
        "# Mini-challenge\n",
        "Let's try to combine the predictions from our previous models. Load the models and scalers from exercises 2 and 3, and use one of the ensemble methods available at the https://scikit-learn.org/stable/modules/ensemble.html . \n",
        "\n",
        "Keep in mind that these ensemble methods will be very useful in our Data Hackaton Challenge. ^^"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiND-WBPtuxp"
      },
      "source": [
        "# Add you code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swWRFAMvBpPJ"
      },
      "source": [
        "# Other references\n",
        "\n",
        "*   https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.02-Introducing-Scikit-Learn.ipynb\n",
        "*   https://scikit-learn.org/stable/modules/classes.html"
      ]
    }
  ]
}