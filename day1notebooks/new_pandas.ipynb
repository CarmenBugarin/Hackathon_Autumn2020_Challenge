{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"Pandas logo\" width=\"100\" src=\"./figures/pandas_secondary.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "<h1>Introduction to Working with Data with Pandas</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting started**\n",
    "\n",
    "\n",
    "<b>pandas</b> (https://pandas.pydata.org/pandas-docs/stable/index.html#) is a software library written for the Python programming language which contains tools for data handling and analysis. Because of its easy-to-use design and broad functionality, it is one of the most popular libraries for data science in Python. It is largely built on top of a more generic and primitive package - NumPy.\n",
    "\n",
    "pandas's stable release version is 1.1.4 as of 30th October 2020.\n",
    "\n",
    "Honestly, pandas's User Guide is a lot more comprehensive and much clearer, (https://pandas.pydata.org/docs/user_guide/index.html) it's worth spending some time there.\n",
    "\n",
    "<br><br>In the <b>\"pandas\"</b> part of this tutorial, we will cover:\n",
    "1. pandas data structures\n",
    "2. input\n",
    "- data selection\n",
    "- adding new data\n",
    "- removing data\n",
    "- data sorting\n",
    "- output\n",
    "- basic methods & attributes to retrieve info\n",
    "\n",
    "<br>In the <b>\"Working with Data\"</b> part of this tutorial, we will cover:\n",
    "9. exploratory data analysis\n",
    "- combining data (concatenate, join, merge)\n",
    "- cleaning data (data types, duplicate & missing data)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkICW-xsFlK0"
   },
   "source": [
    "# **Basic programming skills** for data science\n",
    "For this tutorial, we will use the programming skills covered in the presentation.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/data_science_skills.png\" width=\"400\"></center>\n",
    "<center>(https://blog.udacity.com/2014/11/data-science-job-skills.html)</center>\n",
    "\n",
    "<br>This tutorial aims to give an idea of **data intuition**, **statistics**, **data wrangling**, **visualisation**, and of course, Python/pandas as a popular **programming tool**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap**: Some important terms and concepts..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|terminology|description|\n",
    "|------|------|\n",
    "| <B> library/<br>package </B> | eg. pandas, we often like to set a 'local name' - pd<br><B>pd</B>.read_csv(filepath_or_buffer = \"123.csv\") |\n",
    "|<B> function </B> | pd.<B>read_csv(</B>filepath_or_buffer = \"123.csv\"<B>)</B>|\n",
    "|<B> parameter </B> | pd.read_csv(<B>filepath_or_buffer = </B>\"123.csv\")|\n",
    "|<B> argument </B> | a typical function will require at least an 'input' argument<br>pd.read_csv(filepath_or_buffer = <B>\"123.csv\"</B>)|\n",
    "|<B> variable </B> | <B>df</B> = pd.read_csv(filepath_or_buffer = \"123.csv\")|\n",
    "|<B> object </B> | <b>df</b> is now a 'pandas.DataFrame' object|\n",
    "|<B> attribute </B> | like an ID - df<B>.shape</B>|\n",
    "|<B> method </B> | what we want to do with the content - df<B>.head()</B>, df<B>.head(</B>n = 5<B>)</B>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Importing Python libraries\n",
    "As we saw in the Python course, it is a good practice first to import all the Python libraries that we are going to use in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pandas version\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. pandas data structures\n",
    "pandas is useful because it brings in new data structures that are more convenient to work with. The 2 main structures that we use are:\n",
    "- <b>Series</b> - one-dimensional labelled array that holds data of any type\n",
    "- <b>DataFrame</b> - two-dimensional labelled data structure with columns of potentially different types, like a collection of Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series() is a pandas function for generating a pandas Series object.\n",
    "# We can build a Series object from a list or an array.\n",
    "\n",
    "s = pd.Series([279569, 32906, 25852, 15359], \n",
    "              index = [\"a\", \"b\", \"c\", \"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what type of object is s\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out content of s\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, DataFrame() is a pandas function for making a DataFrame object.\n",
    "# We can build a DataFrame from a dictionary.\n",
    "\n",
    "data = {'region':['England','Scotland','Wales','Northern Ireland'],\n",
    "       'population':[55980000,5440000,3130000,1880000],\n",
    "       'cases':[279569, 32906, 25852, 15359],\n",
    "       'deaths':[36765,2530,1630,585]} # as of 6th October 2020\n",
    "df = pd.DataFrame(data,\n",
    "                 columns = ['region','population','cases','deaths'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use a list of lists instead of a dictionary\n",
    "# for a different input data format.\n",
    "\n",
    "df2 = pd.DataFrame([['England',55980000,279569,36765],\n",
    "                   ['Scotland',5440000,32906,2530],\n",
    "                   ['Wales',3130000,25852,1630],\n",
    "                   ['Northern Ireland',1880000,15359,585]],\n",
    "                  columns = ['region','population','cases','deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input\n",
    "pandas comes with several import functions that can read data from certain types of files directly into a pandas DataFrame object. The most common file type is a 'comma-separated values' (csv) file.\n",
    "\n",
    "Now, let's **download and import** the dataset, then have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_url = 'https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/NHANES_1718_ToyData_modified.csv'\n",
    "\n",
    "dataset = pd.read_csv(dataset_url)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkyrDl0l2gVe"
   },
   "source": [
    "<p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/data.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "<br><br><br>This dataset is compiled from US National Health and Nutrition Examination Survey (NHANES).\n",
    "(https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2017)\n",
    "<br>It has been modified for the purpose of this tutorial.\n",
    "\n",
    "In our dataset, we have data from 4753 people telling their:\n",
    "\n",
    "*   demographic background;\n",
    "*   bodily measures;\n",
    "*   answers to health- and habit-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many more parameters, check them out in the documentation.\n",
    "# we can use the help() function.\n",
    "\n",
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for example, we can set the index when importing data\n",
    "dataset = pd.read_csv(dataset_url, index_col='ID')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data selection\n",
    "Often we want to get a single element or a small subset from the table, instead of listing the whole table and scroll, which doesn't make sense with huge datasets. pandas has its own way to select/slice from a Series or DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame (columns)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|description|code|\n",
    "|------|------|\n",
    "|by column name|dataset['age']|\n",
    "|multiple column names|dataset[['age','gender']]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it yourself\n",
    "dataset[['age','gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame (rows)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|description|code|\n",
    "|------|------|\n",
    "|by position using .iloc (eg. start 2nd end before 4th) |dataset.iloc[1:3]|\n",
    "|by index/name using .loc (eg. 93705 if ID is set as index)|dataset.loc[[93705]]|\n",
    "|rows that meet a logical criteria|dataset[dataset.age > 75]|\n",
    "|first n=3 rows|dataset.head(n=3)|\n",
    "|last n=3 rows|dataset.tail(n=3)|\n",
    "|random fraction of rows|dataset.sample(frac=0.001)|\n",
    "|random n rows|dataset.sample(n=3)|\n",
    "|select and order top n entries|dataset.nlargest(3,'height_cm')|\n",
    "|select and order bottom n entries|dataset.nsmallest(3,'height_cm')|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it yourself\n",
    "dataset.loc[[93705]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame (rows & columns)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|description|code|\n",
    "|------|------|\n",
    "|by position (.iloc)|dataset.iloc[1:5,0:3]|\n",
    "|by labels/names/index (.loc)|dataset.loc[93705,['gender','age']]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it yourself\n",
    "dataset.loc[93705,['gender','age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Series** (eg. dataset.iloc[0], dataset.loc[[93705]], dataset.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|description|code|\n",
    "|------|------|\n",
    "|get an element from the 1-dimension Series object by position|dataset.age.iloc[0]|\n",
    "|get an element from Series by our assigned index|dataset.age.loc[93705]|\n",
    "|get multiple elements by position|dataset.age.iloc[0:3]|\n",
    "|get multiple elements by providing a list/array of indices|dataset.age.loc[[93705,93708,93711]]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it yourself\n",
    "dataset.age.loc[[93705,93708,93711]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy.ndarray**\n",
    "<br>Using the method **.values** on pandas.Series return a numpy.ndarray object. This will be useful for packages that depends on numpy, like Scikit-learn for machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset.age.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding new data\n",
    "We will do simple data addition to a DataFrame here. We will touch upon combining DataFrame objects (concatenating, joining and merging) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add a new column simply by 'selecting' a non-existent column and assigning values to it.\n",
    "\n",
    "dataset['other_household_members'] = dataset['household_size'] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check\n",
    "\n",
    "print(dataset[['household_size','other_household_members']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data removal\n",
    ".drop( ) is a simple method for Series and DataFrame objects to drop values. Be careful, <b>there is no undo button</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop from rows, set axis parameter as 0.\n",
    "# but because axis is 0 by default, we don't have to type it.\n",
    "# check help(pd.DataFrame.drop) for details.\n",
    "\n",
    "dataset.drop(93705, inplace=False).head()\n",
    "\n",
    "# here, inplace parameter means whether we act directly on the object in memory\n",
    "# 'False' will output the change on a copy, won't affect df\n",
    "# 'True' will directly change df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as explained above, the original df is unchanged\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice is to use inplace = False and save a copy, eg:\n",
    "\n",
    "- dataset_short = dataset.drop(93705, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop a column, set axis parameter needs to 1\n",
    "\n",
    "dataset.drop(['gender'], inplace=False, axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data sorting\n",
    "We can easily sort data stored in DataFrame or Series objects. Check out the parameters in the below methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can sort by labels/index of a row or column\n",
    "\n",
    "dataset.sort_index(axis=0, ascending=False) # sort row index, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can sort by values across rows or columns\n",
    "\n",
    "dataset.sort_values(by='height_cm', axis=0, ascending=True) # sort rows by height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rank values by row or column\n",
    "\n",
    "dataset.rank(axis=0, ascending=True) # this returns a DataFrame full of ranks comparing rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output\n",
    "After cleaning or manipulating our data, we can output the DataFrame to a file for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to_csv() method saves the DataFrame object to a csv file\n",
    "\n",
    "dataset.to_csv('my_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's confusing, but we can output tab-separated values (tsv)/txt file with .to_csv()\n",
    "\n",
    "dataset.to_csv('my_df.txt', sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to_excel()\n",
    "\n",
    "dataset.to_excel('my_df.xlsx', sheet_name='test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieve DataFrame information\n",
    "Below are some useful methods or attributes to help us understand some basic information about the DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.shape\n",
    "<br>df.index\n",
    "<br>df.columns\n",
    "<br>df.info()\n",
    "<br>df.count()\n",
    "<br>df.sum()\n",
    "<br>df.cumsum()\n",
    "<br>df.min()\n",
    "<br>df.max()\n",
    "<br>df.describe()\n",
    "<br>df.mean()\n",
    "<br>df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it yourself\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YU56yRWZPNV"
   },
   "source": [
    "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/evaluation.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>  **Challenge**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/NHANES_1718_ToyData_modified.csv'\n",
    "dataset = pd.read_csv(dataset_url, index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join at www.kahoot.it\n",
    "<br>It's easier to play on phone, browser also works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Break\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/time_spent.jpg\" width=\"600\"></center>\n",
    "\n",
    "A core component of data science is to acquire raw data and processing it into an analysis-ready form. Generally, data scientists spend <B>80%</B> of their time <B>collecting, formulating and cleaning data</B>. Therefore, this section is very important - you'll learn useful skills to  spot problems and deal with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exploring your data\n",
    "When you've just got a brand new dataset, the first things to do are:\n",
    "- explore and understand the variables and data properties\n",
    "- diagnosing issues such as outliers, missing values, row duplications, wrong data format, and unexpected values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Inspecting your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a missing value? Which column is it in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the dataset with methods & attributes in section 8.\n",
    "# eg. \n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics is an easy way to spot outliers/problematic data. Can you find it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() gives summary statistics\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More summary methods:\n",
    "- .var( )\n",
    "- .std( )\n",
    "- .quantile([0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# eg. dataset.quantile([0.25, 0.75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Visual exploratory data analysis\n",
    "\n",
    "Visualising data is a great way to spot outliers and obvious issues. Once we identified the errors, we can plan steps to clean the data. We can also get a brief idea of patterns and relationships between variables.\n",
    "\n",
    "- histograms - single variable distribution\n",
    "- boxplots - multiple variables, individual distributions\n",
    "- scatter plots - multiple variables, combined distribution\n",
    "- simple statistical analysis (correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(column = \"height_cm\", bins = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(column = \"height_cm\", by = \"gender\", bins = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.boxplot(column = \"age\", by = \"gender\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "# note that this function is .plot.scatter, not .scatter\n",
    "\n",
    "dataset.plot.scatter(\"height_cm\",\"weight_kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing correlation\n",
    "\n",
    "df.corr(method = \"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using NumPy's Pearson correlation coefficient function\n",
    "\n",
    "np.corrcoef(dataset.height_cm, dataset.weight_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combining data for analysis\n",
    "The ability to transform and combine your data is a crucial skill in data science, because your data may not always come in one monolithic file or table for you to load. A large dataset may be broken into separate datasets to facilitate easier storage and sharing. But it's important to be able to run your analysis on a single dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/data1_2.png\" width=\"200\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame({'X1':['a','b','c'],\n",
    "                      'X2':[11.432, 1.303, 99.906]})\n",
    "\n",
    "data2 = pd.DataFrame({'X1':['a','b','d'],\n",
    "                      'X3':[20.784, 'NaN', 20.784]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Merging\n",
    "Combining tables with different variables. There must be a common identifier. If a particular variable is not present for an entry, a missing value will be created.\n",
    "- left: keep entries as in the 'left'/first data frame\n",
    "- right: keep entries as in the 'right'/second data frame\n",
    "- inner: keep entries that is present in both data frames\n",
    "- outer: keep all entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/merge.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(data1, data2, how='left', on='X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Concatenate\n",
    "Simply glue data frames together. This doesn't consider common identifiers. This is useful for gluing together data frames with different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data1, data2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleaning data for analysis\n",
    "Dive into some of the grittier aspects of data cleaning. Learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You'll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Converting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_depressed is a categorical variable, we can change the values to strings\n",
    "\n",
    "dataset['freq_depressed'] = dataset['freq_depressed'].astype('str')\n",
    "\n",
    "dataset.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables can also be assigned the 'category' dtype\n",
    "\n",
    "dataset['gender'] = dataset['gender'].astype('category')\n",
    "\n",
    "dataset.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the category information is saved for the column\n",
    "# this is sometimes useful for analysis packages\n",
    "\n",
    "dataset.gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Dropping duplicate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.DataFrame({'X1':['a','b','b','c'],\n",
    "                      'X2':[11, 12, 12, 13]})\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "\n",
    "data3.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with any column having NA/null data, then check with info()\n",
    "\n",
    "dataset.dropna(inplace=False).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NA/null data with dedicated value, then check with info()\n",
    "\n",
    "dataset.fillna(\"no\", inplace=False).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".fillna( ) can be used to impute missing data with the mean value, for example, as in df.height_cm.fillna(df.height_cm.mean( ))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 recoding data\n",
    "Sometimes we want the values to be coded differently, eg. no-0, yes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values with others\n",
    "\n",
    "dataset.replace(\"no\", 0, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YU56yRWZPNV"
   },
   "source": [
    "## <p><img alt=\"Scikit-learn logo\" height=\"45px\" src=\"https://raw.githubusercontent.com/KHSDTC/Hackathon_Autumn2020_Challenge/master/day1notebooks/figures/evaluation.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>  **Challenge**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the outlier age value to the correct \"33\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new variable \"BMI\" based on height and weight. (BMI = weight(kg) / [height(m) ^2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. DMDEDUC2 represents \"Education Level\" (https://wwwn.cdc.gov/nchs/nhanes/search/default.aspx). Replace the values with strings as described at (https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.htm#DMDEDUC2).\n",
    "\n",
    "    You can do the same for freq_depressed (DLQ140)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which variable correlates highest with freq_worried? Why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. If they get 3 points for controlling weight, 2 points for increasing exercise, 1 point each for reducing diet salt & fat:\n",
    "\n",
    "        5.1 Which man has the highest points?\n",
    "        5.2 Which woman has 5 points?\n",
    "        5.3 How much points does person 93776 have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a \"hexagonal bin plot\" of height and weight (instead of a scatter plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
